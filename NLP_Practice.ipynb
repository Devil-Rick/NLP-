{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP Practice.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOMmYsA7K7eVkPpNGH/RHJD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Devil-Rick/NLP-/blob/main/NLP_Practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Case Conversion"
      ],
      "metadata": {
        "id": "31-NwfvZ4Y1T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "string = 'The quick brown fox jumped over The Big Dog'\n",
        "string"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "LDh8ICZe4YXt",
        "outputId": "39c05800-d6d1-4f9a-c24b-083f959c4b16"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The quick brown fox jumped over The Big Dog'"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# converting text from 1 form to another\n",
        "# generally all text data is converted into small letters for convenience\n",
        "print(f'In small letters \"{string.lower()}\"')\n",
        "print(f'In capital letters \"{string.upper()}\"')\n",
        "print(f'all 1st letter in each word is capital letter \"{string.title()}\"')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JepNOlbn4njd",
        "outputId": "a10cc9f3-11ca-490e-e9db-a52c61560264"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In small letters \"the quick brown fox jumped over the big dog\"\n",
            "In capital letters \"THE QUICK BROWN FOX JUMPED OVER THE BIG DOG\"\n",
            "all 1st letter in each word is capital letter \"The Quick Brown Fox Jumped Over The Big Dog\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# using NLTK "
      ],
      "metadata": {
        "id": "OwmUO343MJjh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "K6Kzog4VF5FJ"
      },
      "outputs": [],
      "source": [
        "import nltk "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kR3u_4S10Hh",
        "outputId": "82ee973f-98fb-4679-ae10-d1d81c0faf03"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization\n",
        "Is the process of `segmenting running` text into sentences and words. In essence, it’s the task of cutting a text into pieces called `tokens`, and at the same time throwing away certain characters, such as punctuation."
      ],
      "metadata": {
        "id": "zMSBKzok50tA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = (\"US unveils world's most powerful supercomputer, beats China. \" \n",
        "               \"The US has unveiled the world's most powerful supercomputer called 'Summit', \" \n",
        "               \"beating the previous record-holder China's Sunway TaihuLight. With a peak performance \"\n",
        "               \"of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, \"\n",
        "               \"which is capable of 93,000 trillion calculations per second. Summit has 4,608 servers, \"\n",
        "               \"which reportedly take up the size of two tennis courts.\")\n",
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "-zdGGDKA7Wfu",
        "outputId": "66454d70-f9a4-4f60-b13b-c165f29b3efc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"US unveils world's most powerful supercomputer, beats China. The US has unveiled the world's most powerful supercomputer called 'Summit', beating the previous record-holder China's Sunway TaihuLight. With a peak performance of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, which is capable of 93,000 trillion calculations per second. Summit has 4,608 servers, which reportedly take up the size of two tennis courts.\""
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenization using NLTK\n",
        "\n",
        "# 1. Sentence tokens (breaking the para into different sentences)\n",
        "nltk.sent_tokenize(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_h2OvvQL2MqB",
        "outputId": "c0e04b0c-622e-4d67-bf82-359faee74609"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"US unveils world's most powerful supercomputer, beats China.\",\n",
              " \"The US has unveiled the world's most powerful supercomputer called 'Summit', beating the previous record-holder China's Sunway TaihuLight.\",\n",
              " 'With a peak performance of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, which is capable of 93,000 trillion calculations per second.',\n",
              " 'Summit has 4,608 servers, which reportedly take up the size of two tennis courts.']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Word tokens (breaking the para into words)\n",
        "nltk.word_tokenize(text)[:15]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7TamrJoA03E",
        "outputId": "11ca7888-b4f9-4a0f-a5a6-0ba6fb1c05e0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['US',\n",
              " 'unveils',\n",
              " 'world',\n",
              " \"'s\",\n",
              " 'most',\n",
              " 'powerful',\n",
              " 'supercomputer',\n",
              " ',',\n",
              " 'beats',\n",
              " 'China',\n",
              " '.',\n",
              " 'The',\n",
              " 'US',\n",
              " 'has',\n",
              " 'unveiled']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stopwords\n",
        "Some very `common words` that appear to provide `little or no value` to the `NLP objective` are `filtered and excluded` from the text to be processed\n",
        "Includes `pronouns` and `prepositions` such as “and”, “the” or “to” in English"
      ],
      "metadata": {
        "id": "aL03WSoLvX8y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "Qbe6cecVsmg9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example of all the stopwords in english\n",
        "stopwords.words('english')[:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gxf0dbx-zgNZ",
        "outputId": "162d963f-ddaf-4132-e971-18b48e1c2050"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'he',\n",
              " 'him',\n",
              " 'his']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stemming\n",
        "This is the process of slicing the `end or the beginning` of words with the intention of `removing affixes`\n",
        "    \n",
        "    Affixes that are attached at the beginning of the word are called prefixes (e.g. “astro” in the word “astrobiology”) and the ones attached at the end of the word are called suffixes (e.g. “ful” in the word “helpful”).\n",
        "\n",
        "The biggest problem with stemming is that it gives no guarantee that the stem word will have a meanimg `For eg:- Finally will be Fina`.\n",
        "\n",
        "Then why we use stemming ?\n",
        "\n",
        "We use `stemming` as it very fast and simple to use.For our `NLP model` if `speed is more important` than grammer stemming is the way to go.\n"
      ],
      "metadata": {
        "id": "ZZOJB8mt1Aex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "4OHzuhNxzqv5"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Stemmer_Lines = PorterStemmer()\n",
        "Lines = nltk.sent_tokenize(text)\n",
        "\n",
        "for i in range(len(Lines)):\n",
        "  words = nltk.word_tokenize(Lines[i])\n",
        "  stem_words = [Stemmer_Lines.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
        "  Lines[i] = \" \".join(stem_words)\n",
        "Lines"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFkiaUuf8gLP",
        "outputId": "10701cb1-e926-442d-f761-47459c7ef7f8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"US unveil world 's power supercomput , beat china .\",\n",
              " \"the US unveil world 's power supercomput call 'summit ' , beat previou record-hold china 's sunway taihulight .\",\n",
              " 'with peak perform 200,000 trillion calcul per second , twice fast sunway taihulight , capabl 93,000 trillion calcul per second .',\n",
              " 'summit 4,608 server , reportedli take size two tenni court .']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lemmetization\n",
        "`Lemmatization` resolves words to their `dictionary form` (known as lemma) for which it requires detailed dictionaries in which the algorithm can look into and link words to their corresponding lemmas.\n",
        "\n",
        "Unlike stemming the words after lemmatization has a dictionary meaning.\n",
        "`For eg:- finally and final becomes final` \n",
        "\n",
        "Lemmatization also takes the `part of speech` of every word as a `parameter` by default all the words are considered as noun."
      ],
      "metadata": {
        "id": "f0i0-YiL-kyT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "2vEmX_Y9-lSc"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# when we use lemmatize with default settings\n",
        "\n",
        "Lemmatizer_Lines = WordNetLemmatizer()\n",
        "Lines = nltk.sent_tokenize(text)\n",
        "\n",
        "for i in range(len(Lines)):\n",
        "  words = nltk.word_tokenize(Lines[i])\n",
        "  stem_words = [Lemmatizer_Lines.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n",
        "  Lines[i] = \" \".join(stem_words)\n",
        "Lines"
      ],
      "metadata": {
        "id": "0GvAxYJw-fml",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a69d11e9-a21b-4dba-f0fc-715876f93d11"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"US unveils world 's powerful supercomputer , beat China .\",\n",
              " \"The US unveiled world 's powerful supercomputer called 'Summit ' , beating previous record-holder China 's Sunway TaihuLight .\",\n",
              " 'With peak performance 200,000 trillion calculation per second , twice fast Sunway TaihuLight , capable 93,000 trillion calculation per second .',\n",
              " 'Summit 4,608 server , reportedly take size two tennis court .']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# using Diffferent parts of speech\n",
        "\n",
        "# difference bet noun , verb\n",
        "print(f\"Cars change to car as it is a noun   ({Lemmatizer_Lines.lemmatize('cars' , 'n')})\")\n",
        "print(f\"Running doesnt change to run as it is a verb   ({Lemmatizer_Lines.lemmatize('running' , 'n')})\")\n",
        "print(f\"Cars doesnt change to car as it is a noun   ({Lemmatizer_Lines.lemmatize('cars' , 'v')})\")\n",
        "print(f\"Running change to run as it is a verb   ({Lemmatizer_Lines.lemmatize('running' , 'v')})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8R6aEX2n_pdz",
        "outputId": "2625ff5a-ecac-4a7b-8941-6775cf60cc16"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cars change to car as it is a noun   (car)\n",
            "Running doesnt change to run as it is a verb   (running)\n",
            "Cars doesnt change to car as it is a noun   (cars)\n",
            "Running change to run as it is a verb   (run)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TO SOLVE THIS PROB IN LEMMATIZATION**\n",
        "1. we must word tokenize the lines\n",
        "2. then use POS tagging (Parts of speech Tagging)\n",
        "3. convert POS tags to wordnet tags\n",
        "4. apply lemmatization"
      ],
      "metadata": {
        "id": "rJDKCmxgCyKK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tag conversion to wordnet\n",
        "from nltk.corpus import wordnet\n",
        "def post_wordnet(words_token):\n",
        "  tag_map = {'j': wordnet.ADJ, 'v': wordnet.VERB, 'n': wordnet.NOUN, 'r': wordnet.ADV}\n",
        "  new_tagged_tokens = [(word, tag_map.get(tag[0].lower(), wordnet.NOUN)) for word, tag in words_token]\n",
        "  return new_tagged_tokens"
      ],
      "metadata": {
        "id": "SFXUP722hi_Z"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Lemmatizer_Lines = WordNetLemmatizer()\n",
        "Lines = nltk.sent_tokenize(text)\n",
        "Line_list = []\n",
        "for i in range(len(Lines)):\n",
        "  words = nltk.word_tokenize(Lines[i])\n",
        "\n",
        "  # using POS tagging\n",
        "  words_token = nltk.pos_tag(words)\n",
        "\n",
        "  # converting to wordnet tag\n",
        "  final_words = post_wordnet(words_token)\n",
        "\n",
        "  # Finally lemmatizing the words and forming final lines\n",
        "  stem_words = [Lemmatizer_Lines.lemmatize(word , tag) for (word , tag) in final_words if word not in set(stopwords.words('english'))]\n",
        "  Lines[i] = \" \".join(stem_words)\n",
        "  Line_list.append(Lines[i])\n",
        "Lines"
      ],
      "metadata": {
        "id": "U3-3pLidCFKt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e940b47-80ee-4384-8e98-f2e84a4f909a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"US unveils world 's powerful supercomputer , beat China .\",\n",
              " \"The US unveil world 's powerful supercomputer call 'Summit ' , beat previous record-holder China 's Sunway TaihuLight .\",\n",
              " 'With peak performance 200,000 trillion calculation per second , twice fast Sunway TaihuLight , capable 93,000 trillion calculation per second .',\n",
              " 'Summit 4,608 server , reportedly take size two tennis court .']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## When to use Stemming and when to use Lemmatization?\n",
        "\n",
        "Stemming is mostly used to index documents in a search engine where as lemmatization is most common in chatbots"
      ],
      "metadata": {
        "id": "kxdzNLxIl6aX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bag of Words\n",
        "Bag of Words model is used to `preprocess the text` by converting it into a bag of words, which keeps a `count of the total occurrences of most frequently used words`.\n",
        "\n",
        "This model can be visualized using a table, which contains the count of words corresponding to the word itself.\n",
        "\n",
        "Major drawback of this model that it gives equal value to every word so sentiment analysis is hard\n",
        "\n",
        "`For eg:-`\n",
        "\n",
        "sent 1 = 'he is a good boy'\n",
        "\n",
        "sent 2 = 'he is a bad boy'\n",
        "    \n",
        "    Here 'good' 'bad' and 'boy' is given equal priority but good and bad should be given more value as they describe how the boy is.\n"
      ],
      "metadata": {
        "id": "uL9gkAbsnflO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "5CXF0IQrnfVC"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's use the Lines after lemmatizing and convert them into bag of words\n",
        "Bow_vector = CountVectorizer() # Bow = Bag of words\n",
        "final_vector = Bow_vector.fit_transform(Line_list).toarray()\n",
        "final_vector"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YV6dMGr1Y7pC",
        "outputId": "a2dad9cc-30b0-49f6-b3f1-440d0df3a525"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1],\n",
              "       [0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,\n",
              "        1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1],\n",
              "       [2, 1, 0, 1, 0, 2, 0, 1, 0, 0, 1, 0, 1, 2, 1, 0, 0, 0, 0, 2, 0, 0,\n",
              "        0, 1, 0, 1, 0, 0, 0, 2, 1, 0, 0, 0, 0, 1, 0],\n",
              "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,\n",
              "        1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = Bow_vector.get_feature_names_out()\n",
        "vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0PjYfn6h-q8",
        "outputId": "5dcda3dd-c0be-438a-fc5f-3116bdcdcdfc"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['000', '200', '608', '93', 'beat', 'calculation', 'call',\n",
              "       'capable', 'china', 'court', 'fast', 'holder', 'peak', 'per',\n",
              "       'performance', 'powerful', 'previous', 'record', 'reportedly',\n",
              "       'second', 'server', 'size', 'summit', 'sunway', 'supercomputer',\n",
              "       'taihulight', 'take', 'tennis', 'the', 'trillion', 'twice', 'two',\n",
              "       'unveil', 'unveils', 'us', 'with', 'world'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(final_vector , columns=vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "id": "q-224xUXnaDo",
        "outputId": "cdc2c14a-571e-4e9a-e2e0-086b2604d6e2"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-38e6fbb8-2a96-4789-b96f-2100fa1f8e89\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>000</th>\n",
              "      <th>200</th>\n",
              "      <th>608</th>\n",
              "      <th>93</th>\n",
              "      <th>beat</th>\n",
              "      <th>calculation</th>\n",
              "      <th>call</th>\n",
              "      <th>capable</th>\n",
              "      <th>china</th>\n",
              "      <th>court</th>\n",
              "      <th>fast</th>\n",
              "      <th>holder</th>\n",
              "      <th>peak</th>\n",
              "      <th>per</th>\n",
              "      <th>performance</th>\n",
              "      <th>powerful</th>\n",
              "      <th>previous</th>\n",
              "      <th>record</th>\n",
              "      <th>reportedly</th>\n",
              "      <th>second</th>\n",
              "      <th>server</th>\n",
              "      <th>size</th>\n",
              "      <th>summit</th>\n",
              "      <th>sunway</th>\n",
              "      <th>supercomputer</th>\n",
              "      <th>taihulight</th>\n",
              "      <th>take</th>\n",
              "      <th>tennis</th>\n",
              "      <th>the</th>\n",
              "      <th>trillion</th>\n",
              "      <th>twice</th>\n",
              "      <th>two</th>\n",
              "      <th>unveil</th>\n",
              "      <th>unveils</th>\n",
              "      <th>us</th>\n",
              "      <th>with</th>\n",
              "      <th>world</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-38e6fbb8-2a96-4789-b96f-2100fa1f8e89')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-38e6fbb8-2a96-4789-b96f-2100fa1f8e89 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-38e6fbb8-2a96-4789-b96f-2100fa1f8e89');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   000  200  608  93  beat  calculation  ...  two  unveil  unveils  us  with  world\n",
              "0    0    0    0   0     1            0  ...    0       0        1   1     0      1\n",
              "1    0    0    0   0     1            0  ...    0       1        0   1     0      1\n",
              "2    2    1    0   1     0            2  ...    0       0        0   0     1      0\n",
              "3    0    0    1   0     0            0  ...    1       0        0   0     0      0\n",
              "\n",
              "[4 rows x 37 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TF-IDF"
      ],
      "metadata": {
        "id": "RFa-fnPVl285"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "7TA4FYRtk8_4"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TFIDF_vector = TfidfVectorizer()\n",
        "final_vector = TFIDF_vector.fit_transform(Line_list).toarray()\n",
        "final_vector"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPWfLTHhmU-f",
        "outputId": "c51755e2-9699-48ae-ad58-1cea3c2a50df"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.        , 0.        , 0.        , 0.362529  ,\n",
              "        0.        , 0.        , 0.        , 0.362529  , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.362529  , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.362529  ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.45982207, 0.362529  ,\n",
              "        0.        , 0.362529  ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.23154214,\n",
              "        0.        , 0.29368184, 0.        , 0.23154214, 0.        ,\n",
              "        0.        , 0.29368184, 0.        , 0.        , 0.        ,\n",
              "        0.23154214, 0.29368184, 0.29368184, 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.23154214, 0.23154214, 0.23154214,\n",
              "        0.23154214, 0.        , 0.        , 0.29368184, 0.        ,\n",
              "        0.        , 0.        , 0.29368184, 0.        , 0.23154214,\n",
              "        0.        , 0.23154214],\n",
              "       [0.36984322, 0.18492161, 0.        , 0.18492161, 0.        ,\n",
              "        0.36984322, 0.        , 0.18492161, 0.        , 0.        ,\n",
              "        0.18492161, 0.        , 0.18492161, 0.36984322, 0.18492161,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.36984322,\n",
              "        0.        , 0.        , 0.        , 0.14579432, 0.        ,\n",
              "        0.14579432, 0.        , 0.        , 0.        , 0.36984322,\n",
              "        0.18492161, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.18492161, 0.        ],\n",
              "       [0.        , 0.        , 0.34056989, 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.34056989,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.34056989, 0.        ,\n",
              "        0.34056989, 0.34056989, 0.26850921, 0.        , 0.        ,\n",
              "        0.        , 0.34056989, 0.34056989, 0.        , 0.        ,\n",
              "        0.        , 0.34056989, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = TFIDF_vector.get_feature_names_out()\n",
        "vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4CsIVt0miAd",
        "outputId": "f6f6428d-f205-4641-becf-e09aad8d00a6"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['000', '200', '608', '93', 'beat', 'calculation', 'call',\n",
              "       'capable', 'china', 'court', 'fast', 'holder', 'peak', 'per',\n",
              "       'performance', 'powerful', 'previous', 'record', 'reportedly',\n",
              "       'second', 'server', 'size', 'summit', 'sunway', 'supercomputer',\n",
              "       'taihulight', 'take', 'tennis', 'the', 'trillion', 'twice', 'two',\n",
              "       'unveil', 'unveils', 'us', 'with', 'world'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(final_vector , columns=vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "id": "3Y1CkPdDmlzQ",
        "outputId": "9faa360f-7cb2-44ee-8027-4c4b34b198b9"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-0bd36689-f20f-4e5e-a06e-a99aae867073\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>000</th>\n",
              "      <th>200</th>\n",
              "      <th>608</th>\n",
              "      <th>93</th>\n",
              "      <th>beat</th>\n",
              "      <th>calculation</th>\n",
              "      <th>call</th>\n",
              "      <th>capable</th>\n",
              "      <th>china</th>\n",
              "      <th>court</th>\n",
              "      <th>fast</th>\n",
              "      <th>holder</th>\n",
              "      <th>peak</th>\n",
              "      <th>per</th>\n",
              "      <th>performance</th>\n",
              "      <th>powerful</th>\n",
              "      <th>previous</th>\n",
              "      <th>record</th>\n",
              "      <th>reportedly</th>\n",
              "      <th>second</th>\n",
              "      <th>server</th>\n",
              "      <th>size</th>\n",
              "      <th>summit</th>\n",
              "      <th>sunway</th>\n",
              "      <th>supercomputer</th>\n",
              "      <th>taihulight</th>\n",
              "      <th>take</th>\n",
              "      <th>tennis</th>\n",
              "      <th>the</th>\n",
              "      <th>trillion</th>\n",
              "      <th>twice</th>\n",
              "      <th>two</th>\n",
              "      <th>unveil</th>\n",
              "      <th>unveils</th>\n",
              "      <th>us</th>\n",
              "      <th>with</th>\n",
              "      <th>world</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0bd36689-f20f-4e5e-a06e-a99aae867073')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0bd36689-f20f-4e5e-a06e-a99aae867073 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0bd36689-f20f-4e5e-a06e-a99aae867073');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   000  200  608  93  beat  calculation  ...  two  unveil  unveils  us  with  world\n",
              "0    0    0    0   0     1            0  ...    0       0        1   1     0      1\n",
              "1    0    0    0   0     1            0  ...    0       1        0   1     0      1\n",
              "2    2    1    0   1     0            2  ...    0       0        0   0     1      0\n",
              "3    0    0    1   0     0            0  ...    1       0        0   0     0      0\n",
              "\n",
              "[4 rows x 37 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# using SPACY"
      ],
      "metadata": {
        "id": "919QscMsMPib"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "PM_pI9v5MTfT"
      },
      "execution_count": 19,
      "outputs": []
    }
  ]
}